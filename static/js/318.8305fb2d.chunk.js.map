{"version":3,"file":"static/js/318.8305fb2d.chunk.js","mappings":"0KAIA,MAoRA,EAtGwBA,KAEpBC,EAAAA,EAAAA,MAAA,WAASC,MAAO,CAACC,QAAS,OAAQC,WAAY,aAAcC,WAAY,OAAOC,SAAA,EAC7EC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,gCACJL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,sJAEwCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,sBAAyB,0EAK1EL,EAAAA,EAAAA,MAAA,MAAAK,SAAA,CAAI,sBAAkBC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,sBAAwB,gBACpDL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,2FACsFC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,sBAAwB,8IAEvHL,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAAEM,cAAe,OAAQC,YAAa,QAASH,SAAA,EACxDL,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAAEQ,aAAc,QAASJ,SAAA,EAClCL,EAAAA,EAAAA,MAAA,UAAAK,SAAA,EAAQC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,eAAiB,+BAAkC,0CAAsCC,EAAAA,EAAAA,KAAA,UAAAD,SAAQ,SAAa,0IAAsIC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,oEAAsE,KAC9UC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WAlLzB,2PAoLNZ,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAAEQ,aAAc,QAASJ,SAAA,EAClCL,EAAAA,EAAAA,MAAA,UAAAK,SAAA,EAAQC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,wBAA0B,mBAAsB,kIAA8HC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,eAAiB,4DACnNC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WA/KhB,2eAiLfZ,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAAEQ,aAAc,QAASJ,SAAA,EAClCL,EAAAA,EAAAA,MAAA,UAAAK,SAAA,EAAQC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,wBAA0B,0BAA6B,ydACrEC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WArM1B,mlBAuMLZ,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAAEQ,aAAc,QAASJ,SAAA,EAClCL,EAAAA,EAAAA,MAAA,UAAAK,SAAA,EAAQC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,eAAiB,qBAAwB,2EAAuEC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,uBAAyB,sDAC7JC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WA5M7B,gIA+MJN,EAAAA,EAAAA,KAAA,KAAAD,SAAG,kLAKHC,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aACpBP,EAAAA,EAAAA,KAAA,MAAAD,SAAI,6BAEJC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,mBACJL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,sCACiCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,YAAc,iGACNC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,mBAAqB,uCAE7EC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,OAAOC,WA3LX,yBA6LhBN,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aAEpBP,EAAAA,EAAAA,KAAA,MAAAD,SAAI,mCACJL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,2CACsCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,qBAAuB,gDACjDC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,sBAAwB,QAEnDC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WAnMpB,wdAqMTN,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aAEpBb,EAAAA,EAAAA,MAAA,MAAAK,SAAA,CAAI,uCAAmCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,aAAe,QAC5DL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,kFAC6EC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,YAAe,4DAC7CC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,aAAgB,MAACC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,qBAAuB,yCAG5GL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,4DACuDC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,uBAAyB,mEACnCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,gCAAkC,UAC5FC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,YAAc,4CAEtBC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WApMpB,2qBAsMTN,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aAEpBb,EAAAA,EAAAA,MAAA,MAAAK,SAAA,CAAI,0BAAsBC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,aAAe,cAC/CL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,gEAC2DC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,aAAe,mCAC3EC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,oBAAsB,yGAEpCC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WA7LpB,svCA+LTN,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aAEpBP,EAAAA,EAAAA,KAAA,MAAAD,SAAI,iDACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,+CAGHC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WArKpB,24CAuKTN,EAAAA,EAAAA,KAAA,MAAIL,MAAO,CAACY,OAAQ,aAEpBP,EAAAA,EAAAA,KAAA,MAAAD,SAAI,2BACJL,EAAAA,EAAAA,MAAA,KAAAK,SAAA,CAAG,6BACwBC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,YAAc,iFACjCC,EAAAA,EAAAA,KAAA,QAAAD,SAAM,sBAAwB,iGAG5CC,EAAAA,EAAAA,KAACI,EAAAA,EAAS,CAACC,SAAS,SAASC,WArIpB,2tC,6ECrIf,MAAME,EAAkB,CACtBC,SAAU,WACVC,IAAK,QACLC,MAAO,QACPf,QAAS,WACTgB,OAAQ,iBACRC,aAAc,MACdC,gBAAiB,UACjBC,MAAO,OACPC,OAAQ,UACRC,SAAU,OACVC,QAAS,GACTC,WAAY,eACZC,OAAQ,GAIJC,EAAiB,CACrBZ,SAAU,WACVa,SAAU,SACVf,OAAQ,UAmDV,EAjDkBgB,IAA+B,IAA9B,SAAElB,EAAQ,WAAEC,GAAYiB,EACzC,MAAOC,EAAYC,IAAiBC,EAAAA,EAAAA,UAAS,QAoB7C,OACEhC,EAAAA,EAAAA,MAAA,OAAKC,MAAO0B,EAAetB,SAAA,EACzBC,EAAAA,EAAAA,KAAA,UACEL,MAAOa,EACPmB,QAtBaC,KACjBC,UAAUC,UAAUC,UAAUzB,GAAY0B,KAAK,KAC7CP,EAAc,WACdQ,WAAW,KACTR,EAAc,SACb,OACFS,MAAMC,IACPC,QAAQC,MAAM,wBAAyBF,GACvCV,EAAc,SACbQ,WAAW,KACVR,EAAc,SACb,QAYDa,YARmBC,GAAMA,EAAEC,cAAc7C,MAAMuB,QAAU,EASzDuB,WARkBF,GAAMA,EAAEC,cAAc7C,MAAMuB,QAAU,GAQ7BnB,SAE1ByB,KAEHxB,EAAAA,EAAAA,KAAC0C,EAAAA,EAAiB,CAChBrC,SAAUA,EAEVV,MAAOgD,EAAAA,EAEPC,YAAa,CACTC,WAAY,QACZ/B,gBAAiB,UACjBD,aAAc,MACdD,OAAQ,kBACVb,SAEDO,EAAWwC,Y","sources":["chapters/language-models/integrating-a-new-model.jsx","components/CodeBlock.jsx"],"sourcesContent":["import React from 'react';\n\nimport CodeBlock from '../../components/CodeBlock'; \n\nconst init = `\ndef __init__(self, supports_system_prompt: bool = True):\n    self.supports_system_prompt = supports_system_prompt`\nconst prepare = `def _prepare_messages(self, prompt: Union[str, List[ChatMessage]]) -> List[ChatMessage]:\n    if isinstance(prompt, str): return [{\"role\": \"user\", \"content\": prompt}]\n    messages = copy.deepcopy(prompt)\n    if not messages or self.supports_system_prompt: return messages\n    if messages[0]['role'] == 'system':\n        system = messages.pop(0)\n        for m in messages:\n            if m['role'] == 'user': m['content'] = f\"{system['content']}. {m['content']}\"; return messages\n        return [{\"role\": \"user\", \"content\": system['content']}]\n    return messages\n`\nconst generate = `\n@abstractmethod\ndef generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n    \"\"\"\n    Generates a response from a given prompt. This method must be implemented by all subclasses.\n    \"\"\"\n    pass`\nconst continue_generate = `\ndef continue_generate(self, dialog_hist: List[ChatMessage], query: str, **kwargs) -> Tuple[str, List[ChatMessage], int, int, int]:\n    dialog_hist.append({\"role\": \"user\", \"content\": query})\n            \n    resp, completion_tokens, prompt_tokens, total_tokens = self.generate(\n        prompt=dialog_hist, \n        **kwargs\n    )\n\n    dialog_hist.append({\"role\": \"assistant\", \"content\": resp})\n\n    return resp, dialog_hist, completion_tokens, prompt_tokens, total_tokens`\nconst prerequisiteCode = `pip install litellm`;\nconst step1Code = `# In custom_models.py\nfrom base_model import BaseLanguageModel, ChatMessage # Assuming your base class is in base_model.py\nfrom typing import List, Tuple, Union\nimport litellm\nimport os\n\n# Set your API keys if not already done in your environment\n# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nclass LiteLLM(BaseLanguageModel):\n    \"\"\"\n    An implementation of BaseLanguageModel using the litellm library.\n    \"\"\"\n    pass # We will fill this in next.`;\nconst step2Code = `class LiteLLM(BaseLanguageModel):\n    \"\"\"\n    An implementation of BaseLanguageModel using the litellm library.\n    \"\"\"\n    def __init__(self, model_name: str, **kwargs):\n        \"\"\"\n        Initializes the LiteLLM model wrapper.\n\n        Args:\n            model_name (str): The model name to be passed to litellm \n                              (e.g., \"gpt-4o\", \"gemini/gemini-1.5-pro-latest\").\n        \"\"\"\n        # Call the parent constructor. Most models proxied by litellm support system prompts.\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs # Store other potential litellm arguments`;\nconst step3Code = `class LiteLLM(BaseLanguageModel):\n    def __init__(self, model_name: str, **kwargs):\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n        \"\"\"\n        Generates a response using the litellm.completion method.\n        \"\"\"\n        # 1. Use the inherited helper to prepare the final message list\n        messages = self._prepare_messages(prompt)\n\n        # Combine kwargs from initialization and this specific call\n        combined_kwargs = {**self.kwargs, **kwargs}\n\n        # 2. Call the litellm API\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            **combined_kwargs\n        )\n\n        # 3. Parse the response object\n        response_text = response.choices[0].message.content\n        usage = response.usage\n        completion_tokens = usage.completion_tokens\n        prompt_tokens = usage.prompt_tokens\n        total_tokens = usage.total_tokens\n        \n        # 4. Return the data in the required tuple format\n        return response_text, completion_tokens, prompt_tokens, total_tokens`;\nconst step4Code = `from base_model import BaseLanguageModel, ChatMessage # Assuming your base class is in base_model.py\nfrom typing import List, Tuple, Union\nimport litellm\n\nclass LiteLLM(BaseLanguageModel):\n    \"\"\"\n    An implementation of BaseLanguageModel using the litellm library.\n    \"\"\"\n    def __init__(self, model_name: str, **kwargs):\n        \"\"\"\n        Initializes the LiteLLM model wrapper.\n\n        Args:\n            model_name (str): The model name to be passed to litellm \n                              (e.g., \"gpt-4o\", \"claude-3-haiku\").\n        \"\"\"\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n        \"\"\"\n        Generates a response using the litellm.completion method.\n        \"\"\"\n        messages = self._prepare_messages(prompt)\n        combined_kwargs = {**self.kwargs, **kwargs}\n\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            **combined_kwargs\n        )\n        \n        response_text = response.choices[0].message.content\n        usage = response.usage\n        \n        return (\n            response_text,\n            usage.completion_tokens,\n            usage.prompt_tokens,\n            usage.total_tokens\n        )`;\nconst step5Code = `# In your main application file\n# Make sure you have set your API key, e.g., os.environ[\"OPENAI_API_KEY\"] = \"...\"\n\nfrom custom_models import LiteLLM\n\n# Initialize the model for OpenAI's gpt-4o\nmy_model = LiteLLM(model_name=\"gpt-4o\", temperature=0.5)\n\n# --- Simple generation ---\nprint(\"--- Simple Generation ---\")\nresp, c_tokens, p_tokens, t_tokens = my_model.generate(\"Hello, what is the capital of France?\")\nprint(f\"Response: {resp}\")\nprint(f\"Tokens Used (Completion/Prompt/Total): {c_tokens}/{p_tokens}/{t_tokens}\\\\n\")\n\n# --- Multi-turn conversation ---\nprint(\"--- Multi-Turn Conversation ---\")\nconversation_history = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise answers.\"}\n]\n\nresp, conversation_history, c_tokens, p_tokens, t_tokens = my_model.continue_generate(\n    dialog_hist=conversation_history,\n    query=\"What is the capital of Japan?\"\n)\n\nprint(f\"Assistant: {resp}\")\n\nresp, conversation_history, c_tokens, p_tokens, t_tokens = my_model.continue_generate(\n    dialog_hist=conversation_history,\n    query=\"And what is its primary international airport?\"\n)\nprint(f\"Assistant: {resp}\")\nprint(\"\\\\nFinal conversation history:\")\nprint(conversation_history)`;\n\n\nconst LiteLLMTutorial = () => {\n  return (\n    <article style={{padding: '1rem', fontFamily: 'sans-serif', lineHeight: '1.6'}}>\n      <h1>Customizing Your Own Model</h1>\n      <p>\n        This guide will walk you through the process of extending our framework to support a new language model. \n        By following the contract defined by the <code>BaseLanguageModel</code>, you can integrate virtually \n        any LLM provider with minimal effort. \n      </p>\n\n      {/* --- 新增章节 --- */}\n      <h2>Understanding the <code>BaseLanguageModel</code> Contract</h2>\n      <p>\n        Before we build our custom class, it's important to understand the contract set by the <code>BaseLanguageModel</code>. When you inherit from this base class, you gain access to several helpful methods and are required to implement only one core method.\n      </p>\n      <ul style={{ listStyleType: 'disc', paddingLeft: '20px' }}>\n        <li style={{ marginBottom: '10px' }}>\n          <strong><code>generate()</code> (Required to Implement):</strong> This is the only abstract method you <strong>must</strong> implement. Its job is to take a prompt, send it to the specific model's API, and return the response in a standardized tuple format: <code>(response_text, completion_tokens, prompt_tokens, total_tokens)</code>.\n          <CodeBlock language=\"python\" codeString={generate} />\n        </li>\n        <li style={{ marginBottom: '10px' }}>\n          <strong><code>continue_generate()</code> (Inherited):</strong> This method, which handles multi-turn conversations, is already fully implemented in the base class. It works by calling the <code>generate()</code> method you create. You get this functionality for free.\n          <CodeBlock language=\"python\" codeString={continue_generate}/>\n        </li>\n        <li style={{ marginBottom: '10px' }}>\n          <strong><code>_prepare_messages()</code> (Inherited Helper):</strong> This is a utility method provided by the base class to help you correctly format the list of messages before sending it to the API. This method automatically adapts to different model capabilities—for example, merging the system prompt into the first user prompt if system prompts are unsupported. It also serves as the centralized place to add custom transformations, such as injecting context, formatting, or filtering messages before sending them to the model.\n          <CodeBlock language=\"python\" codeString={prepare}/>\n        </li>\n        <li style={{ marginBottom: '10px' }}>\n          <strong><code>__init__()</code> (Constructor):</strong> When writing your subclass's constructor, you should remember to call <code>super().__init__()</code> to ensure the base class is properly initialized.\n          <CodeBlock language=\"python\" codeString={init}/>\n        </li>\n      </ul>\n      <p>\n        This design means you only need to focus on the unique API communication logic for your new model, while the framework handles conversation management and data structuring.\n      </p>\n      {/* --- 新增章节结束 --- */}\n\n      <hr style={{margin: '2rem 0'}} />\n      <h2>A Step-by-Step Tutorial</h2>\n      \n      <h3>Prerequisites</h3>\n      <p>\n        Before you begin, ensure you have <code>litellm</code> installed and have configured the necessary \n        API keys for the model you intend to use (e.g., <code>OPENAI_API_KEY</code> in your environment variables).\n      </p>\n      <CodeBlock language=\"bash\" codeString={prerequisiteCode} />\n\n      <hr style={{margin: '2rem 0'}} />\n\n      <h3>Step 1: Create a New Subclass</h3>\n      <p>\n        First, create a new Python file (e.g., <code>custom_models.py</code>) and define a new class \n        that inherits from <code>BaseLanguageModel</code>.\n      </p>\n      <CodeBlock language=\"python\" codeString={step1Code} />\n\n      <hr style={{margin: '2rem 0'}} />\n\n      <h3>Step 2: Implement the Constructor (<code>__init__</code>)</h3>\n      <p>\n        The constructor is where you'll set up any model-specific configurations. For <code>litellm</code>, \n        we need to know which underlying model to call (e.g., <code>\"gpt-4o\"</code>, <code>\"claude-3-haiku\"</code>). \n        We'll pass this as an argument.\n      </p>\n      <p>\n        You must also call the parent class's constructor using <code>super().__init__()</code> to ensure \n        the base class is properly initialized. We can keep <code>supports_system_prompt=True</code> since \n        <code>litellm</code> handles this for most modern models.\n      </p>\n      <CodeBlock language=\"python\" codeString={step2Code} />\n\n      <hr style={{margin: '2rem 0'}} />\n      \n      <h3>Step 3: Implement the <code>generate</code> Method</h3>\n      <p>\n        This is the core of the integration. You must implement the <code>generate</code> method, which is marked \n        as an <code>@abstractmethod</code> in the base class. The implementation should follow the steps described in the new section above.\n      </p>\n      <CodeBlock language=\"python\" codeString={step3Code} />\n      \n      <hr style={{margin: '2rem 0'}} />\n\n      <h3>Step 4: Putting It All Together (Full Code)</h3>\n      <p>\n        Here is the complete, ready-to-use class.\n      </p>\n      <CodeBlock language=\"python\" codeString={step4Code} />\n\n      <hr style={{margin: '2rem 0'}} />\n\n      <h3>Step 5: Usage Example</h3>\n      <p>\n        Now you can use your new <code>LiteLLM</code> class just like any other model that conforms to the framework. \n        Notice that <code>continue_generate</code> works automatically without any extra implementation because \n        it's part of the base class.\n      </p>\n      <CodeBlock language=\"python\" codeString={step5Code} />\n    </article>\n  );\n};\n\nexport default LiteLLMTutorial;","// src/components/CodeBlock.jsx\n\nimport React, { useState } from 'react';\nimport { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';\n// 导入一个浅色主题，例如 'prism'。如果您想完全自定义，也可以不导入任何主题。\nimport { prism } from 'react-syntax-highlighter/dist/esm/styles/prism'; \n// 如果您想使用 vscDarkPlus 但只想改背景，可以导入 vscDarkPlus 并覆盖其背景色\n\n// 按钮的内联样式\nconst copyButtonStyle = {\n  position: 'absolute',\n  top: '0.8em',\n  right: '0.8em',\n  padding: '6px 12px',\n  border: '1px solid #ccc', // 浅色背景下边框颜色需要调整\n  borderRadius: '6px',\n  backgroundColor: '#e0e0e0', // 按钮背景色也调整为浅色\n  color: '#333', // 按钮文字颜色调整为深色\n  cursor: 'pointer',\n  fontSize: '14px',\n  opacity: 0.8,\n  transition: 'opacity 0.2s',\n  zIndex: 1, // 确保按钮在代码块之上\n};\n\n// 容器的样式\nconst containerStyle = {\n  position: 'relative',\n  maxWidth: '1200px', // 设置一个最大宽度，确保代码块不会过宽\n  margin: '0 auto',  // 让它在屏幕上居中显示\n};\nconst CodeBlock = ({ language, codeString }) => {\n  const [buttonText, setButtonText] = useState('Copy');\n\n  const handleCopy = () => {\n    navigator.clipboard.writeText(codeString).then(() => {\n      setButtonText('Copied!');\n      setTimeout(() => {\n        setButtonText('Copy');\n      }, 2000);\n    }).catch(err => {\n      console.error('Failed to copy text: ', err);\n      setButtonText('Error');\n       setTimeout(() => {\n        setButtonText('Copy');\n      }, 2000);\n    });\n  };\n\n  const handleMouseOver = (e) => e.currentTarget.style.opacity = 1;\n  const handleMouseOut = (e) => e.currentTarget.style.opacity = 0.8; // 浅色背景下，初始透明度可以稍微高一点\n\n  return (\n    <div style={containerStyle}>\n      <button \n        style={copyButtonStyle} \n        onClick={handleCopy}\n        onMouseOver={handleMouseOver}\n        onMouseOut={handleMouseOut}\n      >\n        {buttonText}\n      </button>\n      <SyntaxHighlighter \n        language={language} \n        // 使用一个浅色主题，例如 `prism`\n        style={prism} \n        // 覆盖主题的背景色，确保是白色系\n        customStyle={{\n            paddingTop: '2.5em',\n            backgroundColor: '#f8f8f8', // 浅灰色背景，更柔和\n            borderRadius: '8px', // 可以增加一些圆角\n            border: '1px solid #eee', // 增加一个浅边框\n        }}\n      >\n        {codeString.trim()}\n      </SyntaxHighlighter>\n    </div>\n  );\n};\n\nexport default CodeBlock;"],"names":["LiteLLMTutorial","_jsxs","style","padding","fontFamily","lineHeight","children","_jsx","listStyleType","paddingLeft","marginBottom","CodeBlock","language","codeString","margin","copyButtonStyle","position","top","right","border","borderRadius","backgroundColor","color","cursor","fontSize","opacity","transition","zIndex","containerStyle","maxWidth","_ref","buttonText","setButtonText","useState","onClick","handleCopy","navigator","clipboard","writeText","then","setTimeout","catch","err","console","error","onMouseOver","e","currentTarget","onMouseOut","SyntaxHighlighter","prism","customStyle","paddingTop","trim"],"sourceRoot":""}
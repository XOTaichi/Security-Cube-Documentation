"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[81],{5081:(e,t,s)=>{s.r(t),s.d(t,{default:()=>n});var o=s(6633),r=s(579);const n=()=>(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)("h1",{children:"Attacker Introduction"}),(0,r.jsxs)("p",{children:["The ",(0,r.jsx)("code",{children:"AttackerPipeline"})," class, located in ",(0,r.jsx)("code",{children:"SecurityCube/attacker/base.py"}),", serves as an abstract base for building synchronous attack pipelines. It allows attackers to exploit system vulnerabilities, particularly to bypass model safeguards like content moderation. Subclasses implement the ",(0,r.jsx)("code",{children:"run"})," method to generate adversarial prompts based on specific attack goals."]}),(0,r.jsx)("h2",{children:"AttackerPipeline Class Interface"}),(0,r.jsxs)("p",{children:["The ",(0,r.jsx)("code",{children:"AttackerPipeline"})," class defines the following key components:"]}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"File location:"})," ",(0,r.jsx)("code",{children:"SecurityCube/attacker/base.py"})]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Constructor:"})," The ",(0,r.jsx)("code",{children:"__init__"})," method initializes the attacker pipeline."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"run(goals, target):"})," This abstract method must be implemented by subclasses to generate adversarial prompts. It takes in:",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:"goals"}),': A list of attack objectives (e.g., "How to make a bomb").']}),(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:"target"}),": The defender to attack."]})]})]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Returns:"})," The ",(0,r.jsx)("code",{children:"run"})," method must return a list of dictionaries, where each dictionary contains the results of the attack corresponding to each goal."]})]}),(0,r.jsx)(o.A,{language:"python",codeString:'\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass AttackerPipeline(ABC):\n    """\n    Abstract base class for asynchronous attacker pipeline.\n    """\n\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def run(self, goals: List[str], target: Any) -> List[Dict[str, Any]]:\n        """\n        Generates adversarial prompts based on goals.\n\n        Parameters:\n        goals (List[str]): Attack objectives.\n        target (Any): Target model to attack.\n\n        Returns:\n        List[Dict[str, Any]]: List of attack results.\n        """\n        pass  # Abstract method, to be implemented by subclasses.\n\n        # Example result entry:\n        # {\n        #     "original_question":    # The original attack goal or question.\n        #     "adversarial_prompt":   # The adversarial prompt generated.\n        #     "target_answer":        # The model\'s response to the adversarial prompt.\n        #     "prompt_token_cost":    # Token cost for generating the adversarial prompt.\n        #     "completion_token_cost":  # Token cost for the model\'s response.\n        #     "total_token_cost":     # Total token cost (prompt + response).\n        #     "target_prompt_token_cost":  # Token cost for the target model\'s input prompt.\n        #     "target_completion_token_cost":   # Token cost for the target model\'s response.\n        #     "target_total_token_cost": # Total token cost for both input and output of the target model.\n        #     "execution_time": # Time taken to execute the entire attack, from prompt generation to model response.,\n        #     "round_number": # The current iteration or round of the attack process (useful for multi-round attacks).\n        # }\n\n'}),(0,r.jsx)("h2",{children:"Common Types of Jailbreak Attacks"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Logprobe-based methods:"})," These methods use information from log probes or gradients to construct harmful prompts aimed at manipulating the system."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Shuffle-based methods:"})," Attackers apply benign-looking modifications like scrambling token order or inserting separators to evade detection."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"LLM-based methods:"})," External language models (LLMs) are used to generate, refine, or optimize adversarial prompts to exploit weaknesses in the target model."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Multi-round (conversational) attacks:"})," These attacks rely on turn-by-turn dialogue to incrementally bypass safeguards, often making it harder to detect."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Flaw-based attacks:"})," Exploiting specific model vulnerabilities, such as multilingual misalignment, to construct effective jailbreak prompts."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Strategy-based attacks:"})," Psychologically or procedurally manipulative prompts are used to influence the model\u2019s behavior and bypass security measures."]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Template-based attacks:"})," Algorithmic or brute-force generation based on templates or variations, allowing attackers to create a wide range of adversarial prompts."]})]})]})},6633:(e,t,s)=>{s.d(t,{A:()=>c});var o=s(5043),r=s(2210),n=s(2196),a=s(579);const i={position:"absolute",top:"0.8em",right:"0.8em",padding:"6px 12px",border:"1px solid #ccc",borderRadius:"6px",backgroundColor:"#e0e0e0",color:"#333",cursor:"pointer",fontSize:"14px",opacity:.8,transition:"opacity 0.2s",zIndex:1},l={position:"relative",maxWidth:"1200px",margin:"0 auto"},c=e=>{let{language:t,codeString:s}=e;const[c,d]=(0,o.useState)("Copy");return(0,a.jsxs)("div",{style:l,children:[(0,a.jsx)("button",{style:i,onClick:()=>{navigator.clipboard.writeText(s).then(()=>{d("Copied!"),setTimeout(()=>{d("Copy")},2e3)}).catch(e=>{console.error("Failed to copy text: ",e),d("Error"),setTimeout(()=>{d("Copy")},2e3)})},onMouseOver:e=>e.currentTarget.style.opacity=1,onMouseOut:e=>e.currentTarget.style.opacity=.8,children:c}),(0,a.jsx)(r.A,{language:t,style:n.A,customStyle:{paddingTop:"2.5em",backgroundColor:"#f8f8f8",borderRadius:"8px",border:"1px solid #eee"},children:s.trim()})]})}}}]);
//# sourceMappingURL=81.72a2b43f.chunk.js.map
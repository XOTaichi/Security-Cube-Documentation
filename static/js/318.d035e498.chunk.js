"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[318],{3318:(e,n,s)=>{s.r(n),s.d(n,{default:()=>r});s(5043);var t=s(6633),o=s(579);const r=()=>(0,o.jsxs)("article",{style:{padding:"1rem",fontFamily:"sans-serif",lineHeight:"1.6"},children:[(0,o.jsx)("h1",{children:"Customizing Your Own Model"}),(0,o.jsxs)("p",{children:["This guide will walk you through the process of extending our framework to support a new language model. By following the contract defined by the ",(0,o.jsx)("code",{children:"BaseLanguageModel"}),", you can integrate virtually any LLM provider with minimal effort."]}),(0,o.jsxs)("h2",{children:["Understanding the ",(0,o.jsx)("code",{children:"BaseLanguageModel"})," Contract"]}),(0,o.jsxs)("p",{children:["Before we build our custom class, it's important to understand the contract set by the ",(0,o.jsx)("code",{children:"BaseLanguageModel"}),". When you inherit from this base class, you gain access to several helpful methods and are required to implement only one core method."]}),(0,o.jsxs)("ul",{style:{listStyleType:"disc",paddingLeft:"20px"},children:[(0,o.jsxs)("li",{style:{marginBottom:"10px"},children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"generate()"})," (Required to Implement):"]})," This is the only abstract method you ",(0,o.jsx)("strong",{children:"must"})," implement. Its job is to take a prompt, send it to the specific model's API, and return the response in a standardized tuple format: ",(0,o.jsx)("code",{children:"(response_text, completion_tokens, prompt_tokens, total_tokens)"}),".",(0,o.jsx)(t.A,{language:"python",codeString:'\n@abstractmethod\ndef generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n    """\n    Generates a response from a given prompt. This method must be implemented by all subclasses.\n    """\n    pass'})]}),(0,o.jsxs)("li",{style:{marginBottom:"10px"},children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"continue_generate()"})," (Inherited):"]})," This method, which handles multi-turn conversations, is already fully implemented in the base class. It works by calling the ",(0,o.jsx)("code",{children:"generate()"})," method you create. You get this functionality for free.",(0,o.jsx)(t.A,{language:"python",codeString:'\ndef continue_generate(self, dialog_hist: List[ChatMessage], query: str, **kwargs) -> Tuple[str, List[ChatMessage], int, int, int]:\n    dialog_hist.append({"role": "user", "content": query})\n            \n    resp, completion_tokens, prompt_tokens, total_tokens = self.generate(\n        prompt=dialog_hist, \n        **kwargs\n    )\n\n    dialog_hist.append({"role": "assistant", "content": resp})\n\n    return resp, dialog_hist, completion_tokens, prompt_tokens, total_tokens'})]}),(0,o.jsxs)("li",{style:{marginBottom:"10px"},children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"_prepare_messages()"})," (Inherited Helper):"]})," This is a utility method provided by the base class to help you correctly format the list of messages before sending it to the API. This method automatically adapts to different model capabilities\u2014for example, merging the system prompt into the first user prompt if system prompts are unsupported. It also serves as the centralized place to add custom transformations, such as injecting context, formatting, or filtering messages before sending them to the model.",(0,o.jsx)(t.A,{language:"python",codeString:"def _prepare_messages(self, prompt: Union[str, List[ChatMessage]]) -> List[ChatMessage]:\n    if isinstance(prompt, str): return [{\"role\": \"user\", \"content\": prompt}]\n    messages = copy.deepcopy(prompt)\n    if not messages or self.supports_system_prompt: return messages\n    if messages[0]['role'] == 'system':\n        system = messages.pop(0)\n        for m in messages:\n            if m['role'] == 'user': m['content'] = f\"{system['content']}. {m['content']}\"; return messages\n        return [{\"role\": \"user\", \"content\": system['content']}]\n    return messages\n"})]}),(0,o.jsxs)("li",{style:{marginBottom:"10px"},children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"__init__()"})," (Constructor):"]})," When writing your subclass's constructor, you should remember to call ",(0,o.jsx)("code",{children:"super().__init__()"})," to ensure the base class is properly initialized.",(0,o.jsx)(t.A,{language:"python",codeString:"\ndef __init__(self, supports_system_prompt: bool = True):\n    self.supports_system_prompt = supports_system_prompt"})]})]}),(0,o.jsx)("p",{children:"This design means you only need to focus on the unique API communication logic for your new model, while the framework handles conversation management and data structuring."}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsx)("h2",{children:"A Step-by-Step Tutorial"}),(0,o.jsx)("h3",{children:"Prerequisites"}),(0,o.jsxs)("p",{children:["Before you begin, ensure you have ",(0,o.jsx)("code",{children:"litellm"})," installed and have configured the necessary API keys for the model you intend to use (e.g., ",(0,o.jsx)("code",{children:"OPENAI_API_KEY"})," in your environment variables)."]}),(0,o.jsx)(t.A,{language:"bash",codeString:"pip install litellm"}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsx)("h3",{children:"Step 1: Create a New Subclass"}),(0,o.jsxs)("p",{children:["First, create a new Python file (e.g., ",(0,o.jsx)("code",{children:"custom_models.py"}),") and define a new class that inherits from ",(0,o.jsx)("code",{children:"BaseLanguageModel"}),"."]}),(0,o.jsx)(t.A,{language:"python",codeString:'# In custom_models.py\nfrom base_model import BaseLanguageModel, ChatMessage # Assuming your base class is in base_model.py\nfrom typing import List, Tuple, Union\nimport litellm\nimport os\n\n# Set your API keys if not already done in your environment\n# os.environ["OPENAI_API_KEY"] = "your-api-key"\n\nclass LiteLLM(BaseLanguageModel):\n    """\n    An implementation of BaseLanguageModel using the litellm library.\n    """\n    pass # We will fill this in next.'}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsxs)("h3",{children:["Step 2: Implement the Constructor (",(0,o.jsx)("code",{children:"__init__"}),")"]}),(0,o.jsxs)("p",{children:["The constructor is where you'll set up any model-specific configurations. For ",(0,o.jsx)("code",{children:"litellm"}),", we need to know which underlying model to call (e.g., ",(0,o.jsx)("code",{children:'"gpt-4o"'}),", ",(0,o.jsx)("code",{children:'"claude-3-haiku"'}),"). We'll pass this as an argument."]}),(0,o.jsxs)("p",{children:["You must also call the parent class's constructor using ",(0,o.jsx)("code",{children:"super().__init__()"})," to ensure the base class is properly initialized. We can keep ",(0,o.jsx)("code",{children:"supports_system_prompt=True"})," since",(0,o.jsx)("code",{children:"litellm"})," handles this for most modern models."]}),(0,o.jsx)(t.A,{language:"python",codeString:'class LiteLLM(BaseLanguageModel):\n    """\n    An implementation of BaseLanguageModel using the litellm library.\n    """\n    def __init__(self, model_name: str, **kwargs):\n        """\n        Initializes the LiteLLM model wrapper.\n\n        Args:\n            model_name (str): The model name to be passed to litellm \n                              (e.g., "gpt-4o", "gemini/gemini-1.5-pro-latest").\n        """\n        # Call the parent constructor. Most models proxied by litellm support system prompts.\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs # Store other potential litellm arguments'}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsxs)("h3",{children:["Step 3: Implement the ",(0,o.jsx)("code",{children:"generate"})," Method"]}),(0,o.jsxs)("p",{children:["This is the core of the integration. You must implement the ",(0,o.jsx)("code",{children:"generate"})," method, which is marked as an ",(0,o.jsx)("code",{children:"@abstractmethod"})," in the base class. The implementation should follow the steps described in the new section above."]}),(0,o.jsx)(t.A,{language:"python",codeString:'class LiteLLM(BaseLanguageModel):\n    def __init__(self, model_name: str, **kwargs):\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n        """\n        Generates a response using the litellm.completion method.\n        """\n        # 1. Use the inherited helper to prepare the final message list\n        messages = self._prepare_messages(prompt)\n\n        # Combine kwargs from initialization and this specific call\n        combined_kwargs = {**self.kwargs, **kwargs}\n\n        # 2. Call the litellm API\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            **combined_kwargs\n        )\n\n        # 3. Parse the response object\n        response_text = response.choices[0].message.content\n        usage = response.usage\n        completion_tokens = usage.completion_tokens\n        prompt_tokens = usage.prompt_tokens\n        total_tokens = usage.total_tokens\n        \n        # 4. Return the data in the required tuple format\n        return response_text, completion_tokens, prompt_tokens, total_tokens'}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsx)("h3",{children:"Step 4: Putting It All Together (Full Code)"}),(0,o.jsx)("p",{children:"Here is the complete, ready-to-use class."}),(0,o.jsx)(t.A,{language:"python",codeString:'from base_model import BaseLanguageModel, ChatMessage # Assuming your base class is in base_model.py\nfrom typing import List, Tuple, Union\nimport litellm\n\nclass LiteLLM(BaseLanguageModel):\n    """\n    An implementation of BaseLanguageModel using the litellm library.\n    """\n    def __init__(self, model_name: str, **kwargs):\n        """\n        Initializes the LiteLLM model wrapper.\n\n        Args:\n            model_name (str): The model name to be passed to litellm \n                              (e.g., "gpt-4o", "claude-3-haiku").\n        """\n        super().__init__(supports_system_prompt=True)\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n        """\n        Generates a response using the litellm.completion method.\n        """\n        messages = self._prepare_messages(prompt)\n        combined_kwargs = {**self.kwargs, **kwargs}\n\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            **combined_kwargs\n        )\n        \n        response_text = response.choices[0].message.content\n        usage = response.usage\n        \n        return (\n            response_text,\n            usage.completion_tokens,\n            usage.prompt_tokens,\n            usage.total_tokens\n        )'}),(0,o.jsx)("hr",{style:{margin:"2rem 0"}}),(0,o.jsx)("h3",{children:"Step 5: Usage Example"}),(0,o.jsxs)("p",{children:["Now you can use your new ",(0,o.jsx)("code",{children:"LiteLLM"})," class just like any other model that conforms to the framework. Notice that ",(0,o.jsx)("code",{children:"continue_generate"})," works automatically without any extra implementation because it's part of the base class."]}),(0,o.jsx)(t.A,{language:"python",codeString:'# In your main application file\n# Make sure you have set your API key, e.g., os.environ["OPENAI_API_KEY"] = "..."\n\nfrom custom_models import LiteLLM\n\n# Initialize the model for OpenAI\'s gpt-4o\nmy_model = LiteLLM(model_name="gpt-4o", temperature=0.5)\n\n# --- Simple generation ---\nprint("--- Simple Generation ---")\nresp, c_tokens, p_tokens, t_tokens = my_model.generate("Hello, what is the capital of France?")\nprint(f"Response: {resp}")\nprint(f"Tokens Used (Completion/Prompt/Total): {c_tokens}/{p_tokens}/{t_tokens}\\n")\n\n# --- Multi-turn conversation ---\nprint("--- Multi-Turn Conversation ---")\nconversation_history = [\n    {"role": "system", "content": "You are a helpful assistant that provides concise answers."}\n]\n\nresp, conversation_history, c_tokens, p_tokens, t_tokens = my_model.continue_generate(\n    dialog_hist=conversation_history,\n    query="What is the capital of Japan?"\n)\n\nprint(f"Assistant: {resp}")\n\nresp, conversation_history, c_tokens, p_tokens, t_tokens = my_model.continue_generate(\n    dialog_hist=conversation_history,\n    query="And what is its primary international airport?"\n)\nprint(f"Assistant: {resp}")\nprint("\\nFinal conversation history:")\nprint(conversation_history)'})]})},6633:(e,n,s)=>{s.d(n,{A:()=>p});var t=s(5043),o=s(6101),r=s(2196),i=s(579);const a={position:"absolute",top:"0.8em",right:"0.8em",padding:"6px 12px",border:"1px solid #ccc",borderRadius:"6px",backgroundColor:"#e0e0e0",color:"#333",cursor:"pointer",fontSize:"14px",opacity:.8,transition:"opacity 0.2s",zIndex:1},l={position:"relative",maxWidth:"1200px",margin:"0 auto"},p=e=>{let{language:n,codeString:s}=e;const[p,m]=(0,t.useState)("Copy");return(0,i.jsxs)("div",{style:l,children:[(0,i.jsx)("button",{style:a,onClick:()=>{navigator.clipboard.writeText(s).then(()=>{m("Copied!"),setTimeout(()=>{m("Copy")},2e3)}).catch(e=>{console.error("Failed to copy text: ",e),m("Error"),setTimeout(()=>{m("Copy")},2e3)})},onMouseOver:e=>e.currentTarget.style.opacity=1,onMouseOut:e=>e.currentTarget.style.opacity=.8,children:p}),(0,i.jsx)(o.A,{language:n,style:r.A,customStyle:{paddingTop:"2.5em",backgroundColor:"#f8f8f8",borderRadius:"8px",border:"1px solid #eee"},children:s.trim()})]})}}}]);
//# sourceMappingURL=318.d035e498.chunk.js.map
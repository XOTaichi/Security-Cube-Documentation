"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[788],{6633:(e,s,n)=>{n.d(s,{A:()=>p});var t=n(5043),o=n(6101),r=n(2196),a=n(579);const i={position:"absolute",top:"0.8em",right:"0.8em",padding:"6px 12px",border:"1px solid #ccc",borderRadius:"6px",backgroundColor:"#e0e0e0",color:"#333",cursor:"pointer",fontSize:"14px",opacity:.8,transition:"opacity 0.2s",zIndex:1},l={position:"relative",maxWidth:"1200px",margin:"0 auto"},p=e=>{let{language:s,codeString:n}=e;const[p,d]=(0,t.useState)("Copy");return(0,a.jsxs)("div",{style:l,children:[(0,a.jsx)("button",{style:i,onClick:()=>{navigator.clipboard.writeText(n).then(()=>{d("Copied!"),setTimeout(()=>{d("Copy")},2e3)}).catch(e=>{console.error("Failed to copy text: ",e),d("Error"),setTimeout(()=>{d("Copy")},2e3)})},onMouseOver:e=>e.currentTarget.style.opacity=1,onMouseOut:e=>e.currentTarget.style.opacity=.8,children:p}),(0,a.jsx)(o.A,{language:s,style:r.A,customStyle:{paddingTop:"2.5em",backgroundColor:"#f8f8f8",borderRadius:"8px",border:"1px solid #eee"},children:n.trim()})]})}},6788:(e,s,n)=>{n.r(s),n.d(s,{default:()=>r});var t=n(6633),o=n(579);const r=()=>(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("h1",{children:"BaseLanguageModel Introduction"}),(0,o.jsxs)("p",{children:["The ",(0,o.jsx)("code",{children:"BaseLanguageModel"})," class, located in ",(0,o.jsx)("code",{children:"SecurityCube/models/base.py"}),", provides a standardized interface for interacting with various language models. It supports essential methods for message handling and prompt generation, which can be extended for specific LLM implementations like OpenAI and HuggingFace."]}),(0,o.jsx)("h2",{children:"BaseLanguageModel Class Interface"}),(0,o.jsxs)("p",{children:["The ",(0,o.jsx)("code",{children:"BaseLanguageModel"})," class defines the following key components:"]}),(0,o.jsxs)("ul",{children:[(0,o.jsxs)("li",{children:[(0,o.jsx)("strong",{children:"File location:"})," ",(0,o.jsx)("code",{children:"SecurityCube/models/base.py"})]}),(0,o.jsxs)("li",{children:[(0,o.jsx)("strong",{children:"Constructor:"})," The ",(0,o.jsx)("code",{children:"__init__"})," method initializes the base language model and configures whether the model supports system prompts. By default, it assumes support for system prompts."]}),(0,o.jsxs)("li",{children:[(0,o.jsx)("strong",{children:"_prepare_messages(prompt):"})," A helper method that prepares the message list for API calls, handling system prompts correctly based on the model's capabilities."]}),(0,o.jsxs)("li",{children:[(0,o.jsx)("strong",{children:"generate(prompt, **kwargs):"})," An abstract method for generating a response from a given prompt. It must be implemented by all subclasses to provide model-specific functionality."]}),(0,o.jsxs)("li",{children:[(0,o.jsx)("strong",{children:"continue_generate(dialog_hist, query, **kwargs):"})," A method for continuing a conversation by appending a user query and generating the assistant's response, updating the conversation history."]})]}),(0,o.jsx)(t.A,{language:"python",codeString:'\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple, Union, TypedDict\nimport copy\n\nclass ChatMessage(TypedDict):\n    """Represents a single message in a conversation history."""\n    role: str\n    content: str\n\nclass BaseLanguageModel(ABC):\n    """\n    An abstract base class that defines the standard interface for a language model.\n    """\n\n    def __init__(self, supports_system_prompt: bool = True):\n        """\n        Initializes the base model.\n\n        Args:\n            supports_system_prompt (bool): Flag indicating if the model natively supports \n                                           the \'system\' role in messages. Defaults to True.\n        """\n        self.supports_system_prompt = supports_system_prompt\n\n    def _prepare_messages(self, prompt: Union[str, List[ChatMessage]]) -> List[ChatMessage]:\n        """\n        Prepares the message list for the API call, handling system prompts correctly.\n\n        If the model does not support system prompts, it merges the system prompt\n        into the first user message.\n        """\n        if isinstance(prompt, str):\n            return [{"role": "user", "content": prompt}]\n        \n        # Make a copy to avoid modifying the original list passed by the user\n        messages = copy.deepcopy(prompt)\n\n        if not messages or self.supports_system_prompt:\n            return messages\n\n        # Handle unsupported system prompt\n        if messages[0][\'role\'] == \'system\':\n            system_message = messages.pop(0)\n            # Find the first user message to prepend the system prompt to\n            for message in messages:\n                if message[\'role\'] == \'user\':\n                    message[\'content\'] = f"{system_message[\'content\']}\n\n---\n\n{message[\'content\']}"\n                    return messages\n            \n            # If no user message is found, convert the system message into a user message\n            return [{"role": "user", "content": system_message[\'content\']}]\n\n        return messages\n\n    @abstractmethod\n    def generate(self, prompt: Union[str, List[ChatMessage]], **kwargs) -> Tuple[str, int, int, int]:\n        """\n        Generates a response from a given prompt. This method must be implemented by all subclasses.\n        """\n        pass\n\n    def continue_generate(self, dialog_hist: List[ChatMessage], query: str, **kwargs) -> Tuple[str, List[ChatMessage], int, int, int]:\n        """\n        Continues a conversation by appending a new user query and generating a response.\n        """\n        dialog_hist.append({"role": "user", "content": query})\n        \n        resp, completion_tokens, prompt_tokens, total_tokens = self.generate(\n            prompt=dialog_hist, \n            **kwargs\n        )\n        \n        dialog_hist.append({"role": "assistant", "content": resp})\n        \n        return resp, dialog_hist, completion_tokens, prompt_tokens, total_tokens\n'}),(0,o.jsx)("h2",{children:"Component Breakdown"}),(0,o.jsxs)("ul",{children:[(0,o.jsxs)("li",{children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"BaseLanguageModel"}),":"]})," The abstract base class defining the core LLM interface."]}),(0,o.jsxs)("li",{children:[(0,o.jsxs)("strong",{children:[(0,o.jsx)("code",{children:"OpenAIModel"}),", ",(0,o.jsx)("code",{children:"HuggingFaceModel"}),", etc:"]})," Subclasses implementing the actual model-specific logic."]})]})]})}}]);
//# sourceMappingURL=788.205d695d.chunk.js.map